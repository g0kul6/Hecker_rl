{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages/gym/core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from turtle import forward\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from config import actor_lr,critic_lr,gamma,tau,batch_size\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from memory import DDPG_Memory,TD3_Memory\n",
    "from importlib.resources import path\n",
    "\n",
    "\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "path_dir = os.path.abspath(os.getcwd())\n",
    "path_checkpoint = path_dir + \"/checkpoint/\"\n",
    "\n",
    "\n",
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = 'citylearn-2022-starter-kit/data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "class ARGs:\n",
    "    reward_key = 0\n",
    "    device = 'cuda'\n",
    "    epochs = 1000\n",
    "    actor_lr = 1e-4\n",
    "    critic_lr = 1e-4\n",
    "    gamma = 0.99\n",
    "    batch_size = 256\n",
    "    tau = 0.05\n",
    "\n",
    "args = ARGs()\n",
    "\n",
    "\n",
    "os.mkdir(\"KEY\"+str(args.reward_key))\n",
    "env = CityLearnEnv(schema=Constants.schema_path)\n",
    "os.rmdir(\"KEY\"+str(args.reward_key))\n",
    "\n",
    "env.seed(123456)\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "np.random.seed(123456)\n",
    "\n",
    "# if args.algo == \"ddpg\":\n",
    "#     train_ddpg_mlp(env=env,state_dim=env.observation_space[0].shape[0]*5,action_dim=env.action_space[0].shape[0]*5,actor_lr=args.actor_lr,critic_lr=args.critic_lr,tau=args.tau,\n",
    "#                 batch_size=batch_size,device=args.device,random_steps=50,episodes=args.epochs,update_freq=7,gamma=args.gamma, r=args.reward_key)\n",
    "# elif args.algo == \"td3\":\n",
    "#     train_td3_mlp(env=env,state_dim=env.observation_space[0].shape[0]*5,action_dim=env.action_space[0].shape[0]*5,actor_lr=args.actor_lr,critic_lr=args.critic_lr,tau=args.tau,\n",
    "#                 batch_size=batch_size,device=args.device,random_steps=50,episodes=args.epochs,gamma=args.gamma,policy_freq=2, r=args.reward_key)\n",
    "# elif args.algo == \"sac\":\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 total_score: -1725.4086490051982 Building_Score_1: -399.48849464257177 Building_Score_2: -350.1046792282205 Building_Score_3: -331.74388326611313 Building_Score_4: -309.4549764785408 Building_Score_5: -334.6166153897499\n",
      "Episode: 1 total_score: -1703.1657125448726 Building_Score_1: -386.4946630809305 Building_Score_2: -351.5817853532141 Building_Score_3: -332.8322972128328 Building_Score_4: -300.17473756180334 Building_Score_5: -332.0822293360933\n",
      "Episode: 2 total_score: -1752.0865862489754 Building_Score_1: -417.15653170106737 Building_Score_2: -347.3891824679883 Building_Score_3: -343.3952630711773 Building_Score_4: -300.5418460885354 Building_Score_5: -343.6037629202072\n",
      "Episode: 3 total_score: -1729.3804271255729 Building_Score_1: -388.39547020238246 Building_Score_2: -358.83805851853737 Building_Score_3: -315.10920223099316 Building_Score_4: -326.49172608173717 Building_Score_5: -340.5459700919217\n",
      "Episode: 4 total_score: -1718.396925938404 Building_Score_1: -399.8479515752156 Building_Score_2: -349.2679355143772 Building_Score_3: -326.9846855545705 Building_Score_4: -303.6390757758088 Building_Score_5: -338.657277518433\n",
      "Episode: 5 total_score: -1722.8332750989127 Building_Score_1: -388.7624517550275 Building_Score_2: -342.52943234984537 Building_Score_3: -353.1092723283072 Building_Score_4: -310.3103266897498 Building_Score_5: -328.1217919759834\n",
      "Episode: 6 total_score: -1696.1210375412597 Building_Score_1: -411.78840759372184 Building_Score_2: -328.0450704368219 Building_Score_3: -321.33735857051215 Building_Score_4: -309.90857062424624 Building_Score_5: -325.04163031595743\n",
      "Episode: 7 total_score: -1714.5008582731498 Building_Score_1: -424.52921735095936 Building_Score_2: -338.38099091552846 Building_Score_3: -330.34366716038215 Building_Score_4: -299.98635039147655 Building_Score_5: -321.26063245480464\n",
      "Episode: 8 total_score: -1725.5328770864392 Building_Score_1: -400.9383943668473 Building_Score_2: -355.4451169573116 Building_Score_3: -338.6681936390276 Building_Score_4: -298.0622389850206 Building_Score_5: -332.41893313823215\n",
      "Episode: 9 total_score: -1727.8217673552374 Building_Score_1: -417.77900758424175 Building_Score_2: -336.9683750070111 Building_Score_3: -338.3175920796761 Building_Score_4: -305.50780117253964 Building_Score_5: -329.24899151177067\n",
      "Episode: 10 total_score: -1709.8546340881342 Building_Score_1: -401.15287032890893 Building_Score_2: -351.6281576007002 Building_Score_3: -327.6621039406199 Building_Score_4: -296.14418632479317 Building_Score_5: -333.2673158931106\n",
      "Episode: 11 total_score: -1737.5241018891518 Building_Score_1: -405.78594759253485 Building_Score_2: -338.7050569896771 Building_Score_3: -350.53326404894364 Building_Score_4: -316.4530133002994 Building_Score_5: -326.04681995769903\n",
      "Episode: 12 total_score: -1695.197284307734 Building_Score_1: -397.06183004219184 Building_Score_2: -327.125101576945 Building_Score_3: -340.99847905134004 Building_Score_4: -301.8090798147184 Building_Score_5: -328.2027938225378\n",
      "Episode: 13 total_score: -1737.0173087022883 Building_Score_1: -399.01089954263716 Building_Score_2: -340.00948437253237 Building_Score_3: -350.56919266184957 Building_Score_4: -300.1777894191286 Building_Score_5: -347.24994270613945\n",
      "Episode: 14 total_score: -1737.713600832581 Building_Score_1: -395.08083074995477 Building_Score_2: -365.33389774480605 Building_Score_3: -336.451624407741 Building_Score_4: -298.21460232722876 Building_Score_5: -342.63264560284983\n",
      "Episode: 15 total_score: -1746.929477223318 Building_Score_1: -394.5637154225462 Building_Score_2: -358.8730850312241 Building_Score_3: -327.20828139765064 Building_Score_4: -324.1522358879403 Building_Score_5: -342.1321594839587\n",
      "Episode: 16 total_score: -1734.450329053774 Building_Score_1: -399.38655982911916 Building_Score_2: -349.26013810387536 Building_Score_3: -339.4523993846489 Building_Score_4: -300.16288716023575 Building_Score_5: -346.188344575894\n",
      "Episode: 17 total_score: -1711.5100437937815 Building_Score_1: -385.46599051780396 Building_Score_2: -338.3509787734686 Building_Score_3: -337.612817436608 Building_Score_4: -299.2563751815285 Building_Score_5: -350.8238818843733\n",
      "Episode: 18 total_score: -1729.6540107740432 Building_Score_1: -387.31209207664506 Building_Score_2: -353.0957286092262 Building_Score_3: -332.88921536212587 Building_Score_4: -326.29443202940837 Building_Score_5: -330.0625426966388\n",
      "Episode: 19 total_score: -1746.509253531433 Building_Score_1: -399.5806928976593 Building_Score_2: -356.3039600690617 Building_Score_3: -351.26993921380154 Building_Score_4: -301.1061122358098 Building_Score_5: -338.2485491151\n",
      "Episode: 20 total_score: -1729.2026624759399 Building_Score_1: -402.8849553076305 Building_Score_2: -343.5008405644361 Building_Score_3: -345.3459318691366 Building_Score_4: -301.4988571530795 Building_Score_5: -335.972077581656\n",
      "Episode: 21 total_score: -1727.7056573208756 Building_Score_1: -408.4378379196069 Building_Score_2: -339.17062841496823 Building_Score_3: -335.9152398004493 Building_Score_4: -309.5523479432868 Building_Score_5: -334.6296032425636\n",
      "Episode: 22 total_score: -1722.9539448506832 Building_Score_1: -393.9155867952624 Building_Score_2: -361.1448296796832 Building_Score_3: -333.96048544012797 Building_Score_4: -302.5559354423712 Building_Score_5: -331.37710749324054\n",
      "Episode: 23 total_score: -1727.4389762686205 Building_Score_1: -387.5901285189983 Building_Score_2: -346.267063516441 Building_Score_3: -338.5959766447872 Building_Score_4: -304.7383501235796 Building_Score_5: -350.24745746481705\n",
      "Episode: 24 total_score: -1760.4730661272215 Building_Score_1: -384.97568219957617 Building_Score_2: -363.85088285125744 Building_Score_3: -354.3682227367336 Building_Score_4: -312.18128665182627 Building_Score_5: -345.0969916878294\n",
      "Episode: 25 total_score: -1717.398376267686 Building_Score_1: -383.9699216379093 Building_Score_2: -351.08660069537495 Building_Score_3: -335.5260160890784 Building_Score_4: -297.6515991931098 Building_Score_5: -349.1642386522136\n",
      "Episode: 26 total_score: -1717.0115164721929 Building_Score_1: -401.4215025798115 Building_Score_2: -354.5834481265436 Building_Score_3: -324.83556739301144 Building_Score_4: -306.54427786819394 Building_Score_5: -329.6267205046332\n",
      "Episode: 27 total_score: -1734.2796830311604 Building_Score_1: -390.5447322608642 Building_Score_2: -344.9218180832724 Building_Score_3: -338.7974944317302 Building_Score_4: -307.6015236610017 Building_Score_5: -352.4141145942905\n",
      "Episode: 28 total_score: -1752.9529506509887 Building_Score_1: -390.6355542680276 Building_Score_2: -350.3620426783771 Building_Score_3: -341.2506268310248 Building_Score_4: -322.41613575773744 Building_Score_5: -348.2885911158222\n",
      "Episode: 29 total_score: -1686.9890424328225 Building_Score_1: -381.30981855968156 Building_Score_2: -336.2301742724026 Building_Score_3: -321.1274304127757 Building_Score_4: -310.0737818680853 Building_Score_5: -338.24783731987867\n",
      "Episode: 30 total_score: -1717.6807546219752 Building_Score_1: -400.8804793032072 Building_Score_2: -343.80692966735717 Building_Score_3: -337.88398695600773 Building_Score_4: -312.15890234116085 Building_Score_5: -322.9504563542434\n",
      "Episode: 31 total_score: -1728.5520016196112 Building_Score_1: -388.8052405495286 Building_Score_2: -362.31678493000254 Building_Score_3: -335.9082364475437 Building_Score_4: -299.8319859807709 Building_Score_5: -341.6897537117658\n",
      "Episode: 32 total_score: -1699.2853180685427 Building_Score_1: -383.8698541990605 Building_Score_2: -335.48301173915434 Building_Score_3: -339.4918028569299 Building_Score_4: -297.4012854039968 Building_Score_5: -343.03936386940455\n",
      "Episode: 33 total_score: -1707.793906288288 Building_Score_1: -397.4069391044927 Building_Score_2: -336.1691953408245 Building_Score_3: -324.9004612302054 Building_Score_4: -303.67537682465536 Building_Score_5: -345.6419337881088\n",
      "Episode: 34 total_score: -1735.7391070036308 Building_Score_1: -401.17954026403567 Building_Score_2: -347.1873592842016 Building_Score_3: -337.6146039291896 Building_Score_4: -300.29971619826034 Building_Score_5: -349.457887327945\n",
      "Episode: 35 total_score: -1713.387486602053 Building_Score_1: -411.7138146102313 Building_Score_2: -352.25845804343265 Building_Score_3: -324.90420773036163 Building_Score_4: -300.45118186600695 Building_Score_5: -324.05982435201935\n",
      "Episode: 36 total_score: -1714.4749243374474 Building_Score_1: -385.16613809482624 Building_Score_2: -350.3232444179502 Building_Score_3: -328.3039601885698 Building_Score_4: -317.97891426622635 Building_Score_5: -332.7026673698751\n",
      "Episode: 37 total_score: -1740.8307026979746 Building_Score_1: -387.7756651486591 Building_Score_2: -363.64705665656066 Building_Score_3: -344.85786408162704 Building_Score_4: -312.72325800759495 Building_Score_5: -331.8268588035314\n",
      "Episode: 38 total_score: -1716.512751372938 Building_Score_1: -377.64451300816137 Building_Score_2: -347.7809224042133 Building_Score_3: -329.3517640796545 Building_Score_4: -318.2234926790362 Building_Score_5: -343.5120592018742\n",
      "Episode: 39 total_score: -1736.2663859922172 Building_Score_1: -395.48168114545297 Building_Score_2: -360.60380019821025 Building_Score_3: -351.7257114705061 Building_Score_4: -301.2265812315057 Building_Score_5: -327.22861194654286\n",
      "Episode: 40 total_score: -1746.2664240782458 Building_Score_1: -393.6594556907776 Building_Score_2: -362.3613868182425 Building_Score_3: -336.43057841398155 Building_Score_4: -324.54428372164364 Building_Score_5: -329.27071943359954\n",
      "Episode: 41 total_score: -1701.1159902872594 Building_Score_1: -398.906175474032 Building_Score_2: -353.99082275216887 Building_Score_3: -317.66639852605607 Building_Score_4: -316.12798829332763 Building_Score_5: -314.42460524167393\n",
      "Episode: 42 total_score: -1766.5598867071092 Building_Score_1: -415.4276208238821 Building_Score_2: -334.08491419155456 Building_Score_3: -354.8838785120428 Building_Score_4: -322.5192906564564 Building_Score_5: -339.6441825231728\n",
      "Episode: 43 total_score: -1721.9690173178783 Building_Score_1: -402.89116863826746 Building_Score_2: -337.4647932074325 Building_Score_3: -355.76381022651447 Building_Score_4: -298.45403200269607 Building_Score_5: -327.3952132429653\n",
      "Episode: 44 total_score: -1703.4599864268935 Building_Score_1: -405.95657138351265 Building_Score_2: -334.94867872836903 Building_Score_3: -341.55462943822647 Building_Score_4: -274.7846975710076 Building_Score_5: -346.21540930577964\n",
      "Episode: 45 total_score: -1739.7358047129285 Building_Score_1: -403.9099527595069 Building_Score_2: -370.7629395102277 Building_Score_3: -341.7148504277087 Building_Score_4: -293.48035456231906 Building_Score_5: -329.86770745316784\n",
      "Episode: 46 total_score: -1764.3318926428635 Building_Score_1: -410.52400080424763 Building_Score_2: -359.503769035883 Building_Score_3: -347.2889754625837 Building_Score_4: -302.91522074099515 Building_Score_5: -344.09992659915554\n",
      "Episode: 47 total_score: -1735.5388689560143 Building_Score_1: -403.27764810572944 Building_Score_2: -339.24400653854457 Building_Score_3: -337.89651456297725 Building_Score_4: -304.619673178049 Building_Score_5: -350.5010265707148\n",
      "Episode: 48 total_score: -1730.0705101209107 Building_Score_1: -399.26878215513847 Building_Score_2: -340.87510355350105 Building_Score_3: -351.6855628068324 Building_Score_4: -309.777045956563 Building_Score_5: -328.4640156488773\n",
      "Episode: 49 total_score: -1733.863145235983 Building_Score_1: -401.9662336446681 Building_Score_2: -358.593608345893 Building_Score_3: -346.7926595320757 Building_Score_4: -299.3355886075805 Building_Score_5: -327.17505510576746\n",
      "Episode: 50 total_score: -1241.0341227048114 Building_Score_1: -295.68022904399174 Building_Score_2: -261.1215118580105 Building_Score_3: -235.12184649353765 Building_Score_4: -214.27229602281224 Building_Score_5: -234.83823928645936\n",
      "Episode: 51 total_score: -1239.7580505252145 Building_Score_1: -294.3995952334544 Building_Score_2: -260.08311116512584 Building_Score_3: -224.24125032938716 Building_Score_4: -215.7092402330819 Building_Score_5: -245.32485356416478\n",
      "Episode: 52 total_score: -1215.8238136436523 Building_Score_1: -294.66085853547565 Building_Score_2: -261.35027224830367 Building_Score_3: -224.72626556892928 Building_Score_4: -199.6024598251293 Building_Score_5: -235.48395746581505\n",
      "Episode: 53 total_score: -1229.596590060067 Building_Score_1: -292.7210910288577 Building_Score_2: -271.29107194024135 Building_Score_3: -221.3550056459476 Building_Score_4: -198.7977395924006 Building_Score_5: -245.4316818526217\n",
      "Episode: 54 total_score: -1251.9481309374905 Building_Score_1: -296.54765768066534 Building_Score_2: -264.9901765580592 Building_Score_3: -229.52543261946653 Building_Score_4: -217.55631023602172 Building_Score_5: -243.32855384327726\n",
      "Episode: 55 total_score: -1226.7278841767952 Building_Score_1: -293.8342229253585 Building_Score_2: -259.9727795933772 Building_Score_3: -232.14313128629158 Building_Score_4: -204.58731858155738 Building_Score_5: -236.19043179020954\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 213>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m total_steps\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m     marla_env\u001b[39m.\u001b[39;49mtrain_step(env, total_steps)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m     total_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb Cell 3\u001b[0m in \u001b[0;36mMARLA.train_step\u001b[0;34m(self, env, total_steps)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m Q_target \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetBatchFeatures(next_states)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m     attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_extractor(features\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m     Q_ \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_target[i](attn[:,i],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_target[i](attn[:,i]))\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_agents)]\n",
      "\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb Cell 3\u001b[0m in \u001b[0;36mMARLA.getBatchFeatures\u001b[0;34m(self, mult_states)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetBatchFeatures\u001b[39m(\u001b[39mself\u001b[39m, mult_states):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     mult_states \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(mult_states)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/MARLA/main.ipynb#W2sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mconcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextractor[i](torch\u001b[39m.\u001b[39mFloatTensor(mult_states[:,i])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_agents)], \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from turtle import st\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class DDPG_MLP_ACTOR(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,actor_hidden_dim):\n",
    "        super().__init__()\n",
    "        #actor\n",
    "        self.network_actor = nn.Sequential(\n",
    "            nn.Linear(state_dim,actor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(actor_hidden_dim,actor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(actor_hidden_dim,action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self,s):\n",
    "        action = self.network_actor(s)\n",
    "        return action\n",
    "\n",
    "class DDPG_MLP_CRITIC(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,critic_hidden_dim):\n",
    "        super().__init__()\n",
    "        #critic\n",
    "        self.network_critic = nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim,critic_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(critic_hidden_dim,critic_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(critic_hidden_dim,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self,s,a):\n",
    "        q_value = self.network_critic(torch.cat([s, a], 1))\n",
    "        return q_value\n",
    "\n",
    "\n",
    "class STATE_ATTN_EXTRACTOR(nn.Module):\n",
    "    def __init__(self, state_dim, extractor_hidden_dim = 32 , attn_hidden_dim = 32, n_agents = 5, n_heads = 2):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(attn_hidden_dim, n_heads, batch_first=True)  \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        attn_output, attn_output_weights = self.attention (x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "\n",
    "class MARLA():\n",
    "    def __init__(self, state_dim, action_dim, critic_hidden_dim, actor_hidden_dim, extractor_hidden_dim = 32 , attn_hidden_dim = 32, n_agents = 5, n_heads = 2, device = 'cuda', update_freq = 7, random_steps = 50, max_steps=500):\n",
    "\n",
    "        self.critic = [DDPG_MLP_CRITIC(attn_hidden_dim, action_dim, critic_hidden_dim).to(device=device) for i in range(n_agents)]\n",
    "        self.critic_target = [DDPG_MLP_CRITIC(attn_hidden_dim, action_dim, critic_hidden_dim).to(device=device) for i in range(n_agents)]\n",
    "\n",
    "        self.actor = [DDPG_MLP_ACTOR(attn_hidden_dim, action_dim, actor_hidden_dim).to(device=device) for i in range(n_agents)]\n",
    "        self.actor_target = [DDPG_MLP_ACTOR(attn_hidden_dim, action_dim, actor_hidden_dim).to(device=device) for i in range(n_agents)]\n",
    "\n",
    "\n",
    "        for i in range(n_agents):\n",
    "            self.critic_target[i].load_state_dict(self.critic[i].state_dict())\n",
    "            self.actor_target[i].load_state_dict(self.actor[i].state_dict())\n",
    "\n",
    "        self.attn_extractor = STATE_ATTN_EXTRACTOR(state_dim, extractor_hidden_dim, attn_hidden_dim, n_agents, n_heads).to(device)\n",
    "        self.extractor = [nn.Sequential(\n",
    "            nn.Linear(state_dim, extractor_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(extractor_hidden_dim, attn_hidden_dim),\n",
    "\n",
    "        ).to(device) for i in range(n_agents)]    \n",
    "\n",
    "        self.memory = DDPG_Memory(capacity=10000)\n",
    "        self.n_agents = n_agents\n",
    "        self.device = device\n",
    "        self.random_steps = random_steps\n",
    "        self.update_freq = update_freq\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        acotor_params = []\n",
    "        for i in range(n_agents):\n",
    "            acotor_params+=self.actor[i].parameters()\n",
    "\n",
    "        critic_params = []\n",
    "        for i in range(n_agents):\n",
    "            critic_params+=self.critic[i].parameters()\n",
    "\n",
    "        extractor_params = []\n",
    "        for i in range(n_agents):\n",
    "            extractor_params+=self.extractor[i].parameters()\n",
    "\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(acotor_params,lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(critic_params,lr=critic_lr)\n",
    "        self.attn_extractor_optimizer = torch.optim.Adam(self.attn_extractor.parameters(), lr=0.001)\n",
    "        self.extractor_optimizer = torch.optim.Adam(extractor_params, lr=0.001)\n",
    "\n",
    "    def getFeatures(self, states):\n",
    "        return torch.concat([self.extractor[i](torch.FloatTensor(states[i]).to(self.device)).unsqueeze(0).to(self.device) for i in range(self.n_agents)], 0) #[batch, n_agents, n_feature]\n",
    "\n",
    "    def getBatchFeatures(self, mult_states):\n",
    "        mult_states = np.asarray(mult_states)\n",
    "        return torch.concat([self.extractor[i](torch.FloatTensor(mult_states[:,i]).to(self.device)).to(self.device).unsqueeze(1) for i in range(self.n_agents)], 1)\n",
    "\n",
    "    def train_step(self, env, total_steps):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        building_1=[]\n",
    "        building_2=[]\n",
    "        building_3=[]\n",
    "        building_4=[]\n",
    "        building_5=[]\n",
    "\n",
    "        # pbar = tqdm(total=self.max_steps)\n",
    "        while not done:\n",
    "            # pbar.update(1)\n",
    "            action = []\n",
    "            if total_steps < self.random_steps:\n",
    "                action = [([random.uniform(-1,1)]) for _ in range(5)]\n",
    "            else:\n",
    "                #add gaussian noise \n",
    "                features = self.getFeatures(state)\n",
    "                # print(features.shape)\n",
    "                attn = self.attn_extractor(features.to(self.device))\n",
    "\n",
    "                for nth_agent in range(self.n_agents):\n",
    "                    action.append(([(self.actor[nth_agent](attn[nth_agent]).cpu().detach().numpy() + np.random.normal(scale=0.3,size=1)).clip(-1,1)][0].tolist()))\n",
    "\n",
    "            # print(action)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if steps == self.max_steps:\n",
    "                done = True\n",
    "            steps = steps + 1\n",
    "\n",
    "            building_1.append(reward[0])\n",
    "            building_2.append(reward[1])\n",
    "            building_3.append(reward[2])\n",
    "            building_4.append(reward[3])\n",
    "            building_5.append(reward[4])\n",
    "            score = score + reward.sum()\n",
    "            action = [i[0] for i in action]\n",
    "\n",
    "            self.memory.push(state,next_state=next_state,action=torch.FloatTensor(action),reward=torch.FloatTensor(reward).sum(),done=torch.tensor(done))\n",
    "            state = next_state\n",
    "\n",
    "            if total_steps >= self.random_steps and total_steps%self.update_freq == 0:\n",
    "                for _ in range(self.update_freq):\n",
    "                    #learn\n",
    "                    samples = self.memory.sample(batch_size=batch_size)\n",
    "                    next_states = list(samples.next_state)\n",
    "                    states = list(samples.state)\n",
    "                    actions = torch.stack(list(samples.action)).to(device=self.device)\n",
    "                    dones = torch.stack(list(samples.done)).to(device=self.device)\n",
    "                    rewards = torch.stack(list(samples.reward)).to(device=self.device)\n",
    "                    # Target Q\n",
    "                    Q_ = []\n",
    "                    Q_target = []\n",
    "                    with torch.no_grad():\n",
    "                        features = self.getBatchFeatures(next_states)\n",
    "                        attn = self.attn_extractor(features.to(self.device))\n",
    "\n",
    "                        Q_ = [self.critic_target[i](attn[:,i],self.actor_target[i](attn[:,i])).squeeze(dim=1) for i in range(self.n_agents)]\n",
    "                        Q_target = torch.cat([(rewards+ gamma * (~dones) * Q_[i]).unsqueeze(1) for i in range(self.n_agents)],1)\n",
    "\n",
    "                    #critic update\n",
    "                    i=0\n",
    "                    # print(self.critic[i](attn[:,i],actions[:,i].unsqueeze(0)).squeeze(dim=1).shape)\n",
    "                    Q_Value = torch.cat([self.critic[i](attn[:,i],actions[:,i].unsqueeze(1)) for i in range(self.n_agents)],1)\n",
    "\n",
    "                    critic_loss = F.mse_loss(Q_target,Q_Value)\n",
    "                    self.critic_optimizer.zero_grad()\n",
    "                    critic_loss.backward() \n",
    "                    self.critic_optimizer.step()\n",
    "                    # Freeze crtitic network\n",
    "                    for i in range(self.n_agents):\n",
    "                        for param in self.critic[i].parameters():\n",
    "                            param.requires_grad = False\n",
    "                    #actor update\n",
    "\n",
    "                    features = self.getFeatures(next_states)\n",
    "                    attn = self.attn_extractor(features.to(self.device))\n",
    "\n",
    "                    actor_loss = -1 * torch.cat([self.critic[i](attn[:,i],self.actor[i](attn[:,i])) for i in range(self.n_agents)],0).mean()\n",
    "                    self.actor_optimizer.zero_grad()\n",
    "                    self.extractor_optimizer.zero_grad()\n",
    "                    self.attn_extractor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    self.actor_optimizer.step()\n",
    "                    self.extractor_optimizer.step()\n",
    "                    self.attn_extractor_optimizer.step()\n",
    "                    # Unfreeze critic networks\n",
    "                    for i in range(self.n_agents):\n",
    "                        for param in self.critic[i].parameters():\n",
    "                            param.requires_grad = True\n",
    "\n",
    "                    # soft target update by polyak average\n",
    "                    for i in range(self.n_agents):\n",
    "                        for param_critic,target_param_critic,param_actor,target_param_actor in zip(self.critic[i].parameters(),self.critic_target[i].parameters(),self.actor[i].parameters(),self.actor_target[i].parameters()):\n",
    "                            target_param_critic.data.copy_(tau*param_critic.data + (1-tau)*target_param_critic.data)\n",
    "                            target_param_actor.data.copy_(tau*param_actor.data + (1-tau)*target_param_actor.data)\n",
    "\n",
    "        # pbar.close()\n",
    "        print(\"Episode:\",total_steps,\"total_score:\",score,\"Building_Score_1:\",sum(building_1),\"Building_Score_2:\",sum(building_2),\"Building_Score_3:\",sum(building_3),\"Building_Score_4:\",sum(building_4),\"Building_Score_5:\",sum(building_5))\n",
    "\n",
    "\n",
    "marla_env = MARLA(state_dim = 28, action_dim = 1, critic_hidden_dim = 32, actor_hidden_dim = 32, extractor_hidden_dim = 32 , attn_hidden_dim = 32, n_agents = 5, n_heads = 2, device = 'cuda', update_freq = 7, random_steps = 50, max_steps = 500)\n",
    "total_steps=0\n",
    "for i in range(1000):\n",
    "    marla_env.train_step(env, total_steps)\n",
    "    total_steps +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('citylearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "313191744970a64acf9ad27988ec2d9ba12809c20dfc09db0528f2434aeeaf10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
