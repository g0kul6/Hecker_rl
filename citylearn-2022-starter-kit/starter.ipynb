{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from agents.random_agent import random_policy\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema=Constants.schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32)]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28,)\n",
      "(28,)\n",
      "(28,)\n",
      "(28,)\n",
      "(28,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space[0].shape)\n",
    "print(env.observation_space[1].shape)\n",
    "print(env.observation_space[2].shape)\n",
    "print(env.observation_space[3].shape)\n",
    "print(env.observation_space[4].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1 agent 28*5 5 -> good for starting\n",
    "###### Try optimize the current models (Next Tuesday)  \n",
    "###### First Hyperparameter tuning and then use the best hyperparam to change reward function changes(decide upon which kind of method to use)\n",
    "###### List of possible reward function 200-300 epochs  \n",
    "###### 5 agents (28*1) 1\n",
    "###### 5 agents (28*5) 1\n",
    "###### MAIRL\n",
    "\n",
    "###### Sept30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition_DDPG = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'done'))\n",
    "class DDPG_Memory(object):\n",
    "    def __init__(self,capacity):\n",
    "        \"\"\"\n",
    "        Replay buffer for DDPG\n",
    "        INPUT  : state,next_state,action,reward and done\n",
    "        OUTPUT : batch of off-polocy experience\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, next_state, action, reward, done):\n",
    "        self.memory.append(Transition_DDPG(state, next_state, action, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = Transition_DDPG(*zip(*transitions))\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8608503809017438], [0.7856104610265959], [-0.626287361884049], [0.7304368199592801], [-0.0751147526842082]]\n"
     ]
    }
   ],
   "source": [
    "action = [([random.uniform(-1,1)]) for _ in range(5)]\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price_cost': 1.3946967762483233, 'emmision_cost': 1.3744903942174411}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = [([random.uniform(-1,1)]) for _ in range(5)]\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "metrics_t = env.evaluate()\n",
    "metrics = {\"price_cost\": metrics_t[0], \"emmision_cost\": metrics_t[1]}\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30880322261702986, 0.9471223677436154, -0.9593148921909942, -0.6975377997031114, 0.003272554528882532]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = [i[0] for i in action]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('cityclean')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab4b85fd8da7468fb94c846f153cf1de7563caed8c70a6a37128c726391e9312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
