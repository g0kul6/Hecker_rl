{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HcXw8yhGlMrG"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","\n","DataLoader()"]},{"cell_type":"markdown","metadata":{"id":"6u9YGAqTjBQS"},"source":["### Importing citylearn lib and (modify) stablebaseline lib "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"uqOKdRwLoo8H"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/Forbu/CityLearn-1.3.5.git\n","  Cloning https://github.com/Forbu/CityLearn-1.3.5.git to /tmp/pip-req-build-6p7mbap6\n","  Running command git clone -q https://github.com/Forbu/CityLearn-1.3.5.git /tmp/pip-req-build-6p7mbap6\n","  Resolved https://github.com/Forbu/CityLearn-1.3.5.git to commit a08e01372499e94502aea0435d84eddbde81353b\n","Collecting gym==0.24\n","  Downloading gym-0.24.0.tar.gz (694 kB)\n","\u001b[K     |████████████████████████████████| 694 kB 601 kB/s eta 0:00:01\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: matplotlib in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from CityLearn==1.3.5) (3.5.3)\n","Collecting numpy==1.21.6\n","  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[K     |████████████████████████████████| 15.9 MB 1.6 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: pandas==1.3.5 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from CityLearn==1.3.5) (1.3.5)\n","Requirement already satisfied: Pillow==9.2.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from CityLearn==1.3.5) (9.2.0)\n","Requirement already satisfied: simplejson==3.17.6 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from CityLearn==1.3.5) (3.17.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from gym==0.24->CityLearn==1.3.5) (2.1.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from gym==0.24->CityLearn==1.3.5) (0.0.8)\n","Requirement already satisfied: pytz>=2017.3 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from pandas==1.3.5->CityLearn==1.3.5) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from pandas==1.3.5->CityLearn==1.3.5) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5->CityLearn==1.3.5) (1.16.0)\n","Requirement already satisfied: cycler>=0.10 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->CityLearn==1.3.5) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->CityLearn==1.3.5) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->CityLearn==1.3.5) (21.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->CityLearn==1.3.5) (4.34.4)\n","Requirement already satisfied: pyparsing>=2.2.1 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->CityLearn==1.3.5) (3.0.9)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gym: filename=gym-0.24.0-py3-none-any.whl size=790684 sha256=9bb8b3d521faf3ae1e1131dc8ee8e04af861702edf6b708bf5aad3bbcdb01221\n","  Stored in directory: /home/frozenwolf/.var/app/com.visualstudio.code/cache/pip/wheels/2f/a1/b1/5f4110c8943b36e6cdfcc0e5768481639fa042b3c8357e82cc\n","Successfully built gym\n","Installing collected packages: numpy, gym\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.3\n","    Uninstalling numpy-1.22.3:\n","      Successfully uninstalled numpy-1.22.3\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.24.1\n","    Uninstalling gym-0.24.1:\n","      Successfully uninstalled gym-0.24.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","citylearn 1.3.5 requires gym==0.24.1, but you have gym 0.24.0 which is incompatible.\u001b[0m\n","Successfully installed gym-0.24.0 numpy-1.21.6\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting git+https://github.com/Forbu/stable-baselines3.git\n","  Cloning https://github.com/Forbu/stable-baselines3.git to /tmp/pip-req-build-zfzt6sz7\n","  Running command git clone -q https://github.com/Forbu/stable-baselines3.git /tmp/pip-req-build-zfzt6sz7\n","  Resolved https://github.com/Forbu/stable-baselines3.git to commit 30eb474a6d43d713b907cf2436f18a228e330c29\n","Requirement already satisfied: gym==0.24 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from stable-baselines3==1.6.1a0) (0.24.0)\n","Requirement already satisfied: numpy in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from stable-baselines3==1.6.1a0) (1.21.6)\n","Requirement already satisfied: torch>=1.11 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from stable-baselines3==1.6.1a0) (1.12.1)\n","Requirement already satisfied: cloudpickle in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from stable-baselines3==1.6.1a0) (2.1.0)\n","Requirement already satisfied: pandas in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from stable-baselines3==1.6.1a0) (1.3.5)\n","Requirement already satisfied: matplotlib in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from stable-baselines3==1.6.1a0) (3.5.3)\n","Requirement already satisfied: gym-notices>=0.0.4 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from gym==0.24->stable-baselines3==1.6.1a0) (0.0.8)\n","Requirement already satisfied: typing_extensions in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3==1.6.1a0) (4.1.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (9.2.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (4.34.4)\n","Requirement already satisfied: pyparsing>=2.2.1 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (21.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from matplotlib->stable-baselines3==1.6.1a0) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.6.1a0) (1.16.0)\n","Requirement already satisfied: pytz>=2017.3 in /home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages (from pandas->stable-baselines3==1.6.1a0) (2022.2.1)\n","Building wheels for collected packages: stable-baselines3\n","  Building wheel for stable-baselines3 (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for stable-baselines3: filename=stable_baselines3-1.6.1a0-py3-none-any.whl size=166395 sha256=84f2e1fb9b3af8c1add46399a8b2d34df259b83f01afbb178045f9b4e2fdb2b6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cjgsubyh/wheels/a1/0f/ee/157e30014e3c764b89f8c9f6269c53f8f92a68b1431325a9c0\n","Successfully built stable-baselines3\n","Installing collected packages: stable-baselines3\n","Successfully installed stable-baselines3-1.6.1a0\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%git` not found.\n"]}],"source":["%pip install git+https://github.com/Forbu/CityLearn-1.3.5.git\n","%pip install git+https://github.com/Forbu/stable-baselines3.git\n","%git clone http://gitlab.aicrowd.com/adrien_forbu/neurips-2022-citylearn-challenge.git\n","\n","import os\n","path = \"\"\n","os.chdir(path)"]},{"cell_type":"markdown","metadata":{"id":"sbwbDQrdjKNq"},"source":["### import session"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3011,"status":"ok","timestamp":1662622278392,"user":{"displayName":"Adrien Bufort","userId":"14109265999645915407"},"user_tz":-120},"id":"Eh0MogD7lZxo","outputId":"4a5b3dfe-d80a-47b5-d96b-56e8fc92b6fd"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n","/home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n","  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"]}],"source":["# import couple of libs some will be useful\n","import gym\n","import numpy as np\n","from collections import deque\n","import random\n","import re\n","import os\n","import sys\n","import time\n","import json\n","import itertools\n","\n","# import stable_baselines3\n","from stable_baselines3 import PPO, A2C, DDPG, TD3\n","from stable_baselines3.common.utils import set_random_seed\n","\n","from citylearn.citylearn import CityLearnEnv\n","\n","import functools\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCe2JG3OoCDw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7-NsWcBKjYlG"},"source":["### Main tools\n","\n","Here we define the gym environment in a way that stable baseline 3 lib will be able to understand."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OZNvHEKPllsh"},"outputs":[{"name":"stdout","output_type":"stream","text":["['aicrowd.json', 'starter.ipynb', 'local_evaluation.py', 'train.py', 'data', 'ddpg_Analysis.ipynb', 'agents', 'submit.sh', 'requirements.txt', 'drl_algo', 'README.md', 'rewards', 'apt.txt', 'going-below-1-0-score-with-stablebaseline3.ipynb', 'docs']\n"]}],"source":["\n","\n","class Constants:\n","    episodes = 3\n","    schema_path = 'data/citylearn_challenge_2022_phase_1/schema.json'\n","\n","\n","def action_space_to_dict(aspace):\n","    \"\"\" Only for box space \"\"\"\n","    return { \"high\": aspace.high,\n","             \"low\": aspace.low,\n","             \"shape\": aspace.shape,\n","             \"dtype\": str(aspace.dtype)\n","    }\n","\n","def env_reset(env):\n","    observations = env.reset()\n","    action_space = env.action_space\n","    observation_space = env.observation_space\n","    building_info = env.get_building_information()\n","    building_info = list(building_info.values())\n","    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n","    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n","    obs_dict = {\"action_space\": action_space_dicts,\n","                \"observation_space\": observation_space_dicts,\n","                \"building_info\": building_info,\n","                \"observation\": observations }\n","    return obs_dict\n","\n","import gym\n","\n","# here we init the citylearn env\n","env = CityLearnEnv(schema=Constants.schema_path)\n","\n","#### IMPORTANT \n","# here we choose the observation we want to take from the building env\n","# we divide observation that are specific to buildings (index_particular)\n","# and observation that are the same for all the buildings (index_commun)\n","\n","index_commun = [0, 2, 19, 4, 8, 24]\n","index_particular = [20, 21, 22, 23]\n","\n","normalization_value_commun = [12, 24, 2, 100, 100, 1]\n","normalization_value_particular = [5, 5, 5, 5]\n","\n","len_tot_index = len(index_commun) + len(index_particular) * 5\n","\n","## env wrapper for stable baselines\n","class EnvCityGym(gym.Env):\n","    \"\"\"\n","    Env wrapper coming from the gym library.\n","    \"\"\"\n","    def __init__(self, env):\n","        self.env = env\n","\n","        # get the number of buildings\n","        self.num_buildings = len(env.action_space)\n","\n","        # define action and observation space\n","        self.action_space = gym.spaces.Box(low=np.array([-1] * self.num_buildings), high=np.array([1] * self.num_buildings), dtype=np.float32)\n","\n","        # define the observation space\n","        self.observation_space = gym.spaces.Box(low=np.array([0] * len_tot_index), high=np.array([1] * len_tot_index), dtype=np.float32)\n","\n","        # TO THINK : normalize the observation space\n","\n","    def reset(self):\n","        obs_dict = env_reset(self.env)\n","        obs = self.env.reset()\n","\n","        observation = self.get_observation(obs)\n","\n","        return observation\n","\n","    def get_observation(self, obs):\n","        \"\"\"\n","        We retrieve new observation from the building observation to get a proper array of observation\n","        Basicly the observation array will be something like obs[0][index_commun] + obs[i][index_particular] for i in range(5)\n","\n","        The first element of the new observation will be \"commun observation\" among all building like month / hour / carbon intensity / outdoor_dry_bulb_temperature_predicted_6h ...\n","        The next element of the new observation will be the concatenation of certain observation specific to buildings non_shiftable_load / solar_generation / ...  \n","        \"\"\"\n","        \n","        # we get the observation commun for each building (index_commun)\n","        observation_commun = [obs[0][i]/n for i, n in zip(index_commun, normalization_value_commun)]\n","        observation_particular = [[o[i]/n for i, n in zip(index_particular, normalization_value_particular)] for o in obs]\n","\n","        observation_particular = list(itertools.chain(*observation_particular))\n","        # we concatenate the observation\n","        observation = observation_commun + observation_particular\n","\n","        return observation\n","\n","    def step(self, action):\n","        \"\"\"\n","        we apply the same action for all the buildings\n","        \"\"\"\n","        # reprocessing action\n","        action = [[act] for act in action]\n","\n","        # we do a step in the environment\n","        obs, reward, done, info = self.env.step(action)\n","\n","        observation = self.get_observation(obs)\n","\n","        return observation, sum(reward), done, info\n","        \n","    def render(self, mode='human'):\n","        return self.env.render(mode)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GCzGbZmSlYg0"},"source":["### Train and test function\n","\n","The function to train and test the sb3 PPO algorithm"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"SrJsJWxjjj4g"},"outputs":[],"source":["\n","# function to train the policy with PPO algorithm\n","def test_ppo():\n","\n","    # Modify the petting zoo environment to make a custom observation space (return an array of value for each agent)\n","    \n","\n","    # first we initialize the environment (petting zoo)\n","    env = CityLearnEnv(schema=Constants.schema_path)\n","    env = EnvCityGym(env)\n","    \n","    # we load the model\n","    model = PPO.load(\"ppo_citylearn\")\n","\n","    # we reset the environment\n","    obs = env.reset()\n","\n","    nb_iter = 8000\n","\n","    # loop on the number of iteration\n","    for i in range(nb_iter):\n","        # we get the action for each agent\n","        actions = []\n","        for agent in env.possible_agents:\n","            action, _states = model.predict(obs[agent], deterministic=True)\n","\n","\n","            actions.append(action)\n","\n","        actions = {agent: action for agent, action in zip(env.possible_agents, actions)}\n","\n","        # we do a step in the environment\n","        obs, rewards, dones, info = env.step(actions)\n","\n","        # sometimes check the actions and rewards\n","        if i % 100 == 0:\n","            print(\"actions : \", actions)\n","            print(\"rewards : \", rewards)\n","\n","        \n","\n","\n","    final_result = sum(env.citylearnenv.evaluate())/2\n","\n","    print(\"final result : \", final_result)\n","    # launch as main\n","\n","    return final_result\n","    \n","\n","# function to train the policy with PPO algorithm\n","def train_ppo():\n","\n","    # first we initialize the environment (petting zoo)\n","    env = CityLearnEnv(schema=Constants.schema_path)\n","    env = EnvCityGym(env)\n","\n","    env.reset()\n","\n","    # Configure the algorithm\n","\n","    # load model if exist\n","    \n","    # model = PPO.load(\"ppo_citylearn\")\n","    # except:\n","    model = PPO('MlpPolicy', env, verbose=2, gamma=0.99)\n","\n","    # Train the agent\n","    model.learn(total_timesteps=10000000)\n","\n","    model.save(\"ppo_citylearn\")\n","\n","    return model\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29120254,"status":"ok","timestamp":1662651399502,"user":{"displayName":"Adrien Bufort","userId":"14109265999645915407"},"user_tz":-120},"id":"U6DiqRA1lpsH","outputId":"2e699b43-ca8e-47be-b30d-989f385c537a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n"]},{"name":"stderr","output_type":"stream","text":["/home/frozenwolf/miniconda3/envs/citylearn/lib/python3.10/site-packages/gym/spaces/box.py:112: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"]},{"ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a real number, not 'list'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train_ppo()\n","\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10000000\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mppo_citylearn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:310\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    311\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    312\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    313\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    314\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    315\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    316\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    317\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    318\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    319\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    320\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[39mbreak\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:176\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    174\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 176\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    180\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n","\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb Cell 11\u001b[0m in \u001b[0;36mEnvCityGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m action \u001b[39m=\u001b[39m [[act] \u001b[39mfor\u001b[39;00m act \u001b[39min\u001b[39;00m action]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# we do a step in the environment\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_observation(obs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X14sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mreturn\u001b[39;00m observation, \u001b[39msum\u001b[39m(reward), done, info\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/citylearn/citylearn.py:455\u001b[0m, in \u001b[0;36mCityLearnEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    452\u001b[0m     building\u001b[39m.\u001b[39mapply_actions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuilding_actions)\n\u001b[1;32m    454\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_time_step()\n\u001b[0;32m--> 455\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_reward()\n\u001b[1;32m    456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m    457\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations, reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_info()\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/citylearn/citylearn.py:474\u001b[0m, in \u001b[0;36mCityLearnEnv.get_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_function\u001b[39m.\u001b[39mcarbon_emission \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__net_electricity_consumption_emission[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_step]] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentral_agent\\\n\u001b[1;32m    471\u001b[0m     \u001b[39melse\u001b[39;00m [b\u001b[39m.\u001b[39mnet_electricity_consumption_emission[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_step] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuildings]\n\u001b[1;32m    472\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_function\u001b[39m.\u001b[39melectricity_price \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__net_electricity_consumption_price[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_step]] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentral_agent\\\n\u001b[1;32m    473\u001b[0m     \u001b[39melse\u001b[39;00m [b\u001b[39m.\u001b[39mnet_electricity_consumption_price[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_step] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuildings]\n\u001b[0;32m--> 474\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_function\u001b[39m.\u001b[39;49mcalculate()\n\u001b[1;32m    475\u001b[0m \u001b[39mreturn\u001b[39;00m reward\n","File \u001b[0;32m~/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/rewards/user_reward.py:42\u001b[0m, in \u001b[0;36mUserReward.calculate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m     35\u001b[0m     \u001b[39m\"\"\"CityLearn Challenge reward calculation.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m    Notes\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    -----\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    This function is called internally in the environment's :meth:`citylearn.CityLearnEnv.step` function.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m get_reward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melectricity_consumption, \n\u001b[1;32m     43\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcarbon_emission, \n\u001b[1;32m     44\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melectricity_price, \n\u001b[1;32m     45\u001b[0m                       \u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent_count)))\n","File \u001b[0;32m~/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/rewards/get_reward.py:53\u001b[0m, in \u001b[0;36mget_reward\u001b[0;34m(electricity_consumption, carbon_emission, electricity_price, agent_ids)\u001b[0m\n\u001b[1;32m     50\u001b[0m electricity_price \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(np\u001b[39m.\u001b[39marray(electricity_price))\n\u001b[1;32m     51\u001b[0m electricity_consumption \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(np\u001b[39m.\u001b[39masarray(electricity_consumption))\n\u001b[0;32m---> 53\u001b[0m reward \u001b[39m=\u001b[39m custom_rewards(carbon_emission \u001b[39m+\u001b[39m electricity_price,\u001b[39mint\u001b[39;49m(keys_store\u001b[39m.\u001b[39;49mreward_key))\n\u001b[1;32m     55\u001b[0m \u001b[39m# print(\"CARBON EMISSION: \",carbon_emission)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# print(\"ELECTRICITY PRICE: \",electricity_price)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# print(\"ELECTRICITY CONSUMPTION: \",electricity_consumption)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# print(\"REWARD: \",reward, end=\"\\n\\n\")\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# ************** END ***************\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m reward\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"]}],"source":["model = train_ppo()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VddV2zrJEC28"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16577,"status":"ok","timestamp":1662651416071,"user":{"displayName":"Adrien Bufort","userId":"14109265999645915407"},"user_tz":-120},"id":"x6wWwpr0noj4","outputId":"427b78d0-4a54-4d99-9e05-4074863a2fac"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'ppo_citylearn.zip'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m EnvCityGym(env)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mppo_citylearn\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m nb_iter \u001b[39m=\u001b[39m \u001b[39m8750\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/frozenwolf/Desktop/neurIPS/Hecker_rl/citylearn-2022-starter-kit/going-below-1-0-score-with-stablebaseline3.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m reward_tot \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:705\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    703\u001b[0m     get_system_info()\n\u001b[0;32m--> 705\u001b[0m data, params, pytorch_variables \u001b[39m=\u001b[39m load_from_zip_file(\n\u001b[1;32m    706\u001b[0m     path, device\u001b[39m=\u001b[39;49mdevice, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, print_system_info\u001b[39m=\u001b[39;49mprint_system_info\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# Remove stored device information and replace with ours\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpolicy_kwargs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m data:\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:388\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_zip_file\u001b[39m(\n\u001b[1;32m    362\u001b[0m     load_path: Union[\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath, io\u001b[39m.\u001b[39mBufferedIOBase],\n\u001b[1;32m    363\u001b[0m     load_data: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m     print_system_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    368\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (Tuple[Optional[Dict[\u001b[39mstr\u001b[39m, Any]], Optional[TensorDict], Optional[TensorDict]]):\n\u001b[1;32m    369\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    Load model data from a .zip archive\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m        and dict of pytorch variables\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m     load_path \u001b[39m=\u001b[39m open_path(load_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49mverbose, suffix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    390\u001b[0m     \u001b[39m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     device \u001b[39m=\u001b[39m get_device(device\u001b[39m=\u001b[39mdevice)\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:232\u001b[0m, in \u001b[0;36mopen_path_str\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m@open_path\u001b[39m\u001b[39m.\u001b[39mregister(\u001b[39mstr\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_path_str\u001b[39m(path: \u001b[39mstr\u001b[39m, mode: \u001b[39mstr\u001b[39m, verbose: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, suffix: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m io\u001b[39m.\u001b[39mBufferedIOBase:\n\u001b[1;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m    that the path exists.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mreturn\u001b[39;00m open_path(pathlib\u001b[39m.\u001b[39;49mPath(path), mode, verbose, suffix)\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    277\u001b[0m         path\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    279\u001b[0m \u001b[39m# if opening was successful uses the identity function\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39m#   with corrections\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m open_path(path, mode, verbose, suffix)\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    262\u001b[0m             path, suffix \u001b[39m=\u001b[39m newpath, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m             \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:256\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m         path \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39;49mopen(\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    257\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    258\u001b[0m         \u001b[39mif\u001b[39;00m suffix \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m suffix \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/citylearn/lib/python3.10/pathlib.py:1117\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1116\u001b[0m     encoding \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1117\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors,\n\u001b[1;32m   1118\u001b[0m                            newline)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ppo_citylearn.zip'"]}],"source":["# simple run though the env with our PPO policy and we sometimes print our actions / reward to get a sense of what we are doing\n","env = CityLearnEnv(schema=Constants.schema_path)\n","env = EnvCityGym(env)\n","\n","obs = env.reset()\n","\n","model = PPO.load(\"ppo_citylearn\")\n","\n","nb_iter = 8750\n","\n","reward_tot = 0\n","\n","for i in range(nb_iter):\n","\n","    action = model.predict(obs)[0]\n","        \n","    obs, rewards, dones, info = env.step(action)\n","    reward_tot += rewards \n","\n","    if i % 1000 == 0:\n","        print(\"actions : \", action)\n","        print(\"rewards : \", rewards)\n","\n","print(sum(env.env.evaluate())/2)\n","print(reward_tot)"]},{"cell_type":"markdown","metadata":{"id":"mFonvZmxllig"},"source":["# GUYS WE ARE AT 0.95 ! BELOW 1.0 !"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPR6LvDNXXprR8HyxmX4N5g","collapsed_sections":[],"provenance":[{"file_id":"1nVaSlhoXP6-csOuDxU6IE37BdPca_x3F","timestamp":1662403569200}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.4 ('citylearn')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"313191744970a64acf9ad27988ec2d9ba12809c20dfc09db0528f2434aeeaf10"}}},"nbformat":4,"nbformat_minor":0}
